{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0a85f68a",
      "metadata": {
        "id": "0a85f68a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a395137-446e-4485-edb3-3b2999db474e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mido\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▍                         | 10 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 20 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 30 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 40 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51 kB 5.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: mido\n",
            "Successfully installed mido-1.2.10\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# if using google colab - set up path properly\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    !pip install mido\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "\n",
        "    import sys\n",
        "    cwd = '/content/drive/My Drive/school/stat4984/final_proj/'\n",
        "    sys.path.append(cwd)\n",
        "\n",
        "else:\n",
        "    cwd = os.getcwd()+'/'\n",
        "\n",
        "\n",
        "import mido\n",
        "from midi_ndarrays import *\n",
        "\n",
        "# set up directories\n",
        "midi_data_dir = cwd+'midi_data/'\n",
        "csv_data_dir  = cwd+'csv_data/'\n",
        "\n",
        "if not os.path.exists(midi_data_dir):\n",
        "    os.makedirs(midi_data_dir)\n",
        "\n",
        "if not os.path.exists(midi_data_dir):\n",
        "    os.makedirs(csv_data_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c33f11e6",
      "metadata": {
        "id": "c33f11e6"
      },
      "outputs": [],
      "source": [
        "def dir_idx(dir_name, n):\n",
        "    \"\"\"\n",
        "    return the name of the nth file in a directory\n",
        "    \"\"\"\n",
        "    \n",
        "    return dir_name+os.listdir(dir_name)[n]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caed195a",
      "metadata": {
        "id": "caed195a"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dabc932d",
      "metadata": {
        "scrolled": true,
        "id": "dabc932d"
      },
      "outputs": [],
      "source": [
        "# only download midis if the batch dir doesn't exist and midi dir is empty\n",
        "if len(os.listdir(midi_data_dir)) == 0 and len(os.listdir(csv_data_dir)) == 0:\n",
        "    from midi_utils import download_midis\n",
        "    download_midis(midi_data_dir)\n",
        "\n",
        "\n",
        "def load_array(midi_filename):\n",
        "    \"\"\"\n",
        "    return midi_filename as a ndarray with start and end tokens\n",
        "    Replace all notes with 1 - played, or 0 - not played\n",
        "    \"\"\"\n",
        "    midi_tracks = mido.MidiFile(midi_filename, clip=True)\n",
        "    midi_array = mid2array(midi_tracks)\n",
        "    \n",
        "    # set all values to 1 or 0\n",
        "    midi_array = np.where(midi_array != 0, 1, 0).astype('uint8')\n",
        "    \n",
        "    # add padding and encode start token (first column)\n",
        "    # and end token (last column)\n",
        "    midi_array = np.pad(midi_array, 1)\n",
        "    midi_array[0, 0]   = 1\n",
        "    midi_array[-1, -1] = 1\n",
        "    \n",
        "    return midi_array\n",
        "\n",
        "# if there aren't any csv files, convert all midis arrays and save in csv\n",
        "if len(os.listdir(csv_data_dir)) == 0:\n",
        "    for i in range(len(os.listdir(midi_data_dir))):\n",
        "        midi_filename = dir_idx(midi_data_dir, i)\n",
        "        midi_array = load_array(midi_filename)\n",
        "        np.savetxt(midi_filename[:-4]+\".csv\", midi_array, fmt=\"%d\", delimiter=\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4f7a8b",
      "metadata": {
        "id": "1c4f7a8b"
      },
      "source": [
        "# The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8f0840d8",
      "metadata": {
        "id": "8f0840d8"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout_p):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_size  = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers  = num_layers\n",
        "        \n",
        "        self.dropout   = nn.Dropout(dropout_p)\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn       = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout_p)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # input dimension [1, 90]\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        _, (h, c) = self.rnn(embedding)\n",
        "        return h, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "69d55259",
      "metadata": {
        "id": "69d55259"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, \n",
        "                num_layers, dropout_p):\n",
        "    \n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers  = num_layers\n",
        "\n",
        "        self.dropout   = nn.Dropout(dropout_p)\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        \n",
        "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout_p)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x, hidden, cell):\n",
        "        x = x.unsqueeze(0)\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "        predictions = self.fc(outputs.squeeze(0))\n",
        "\n",
        "        # sigmoid because we're predicting multiple notes\n",
        "        predictions = torch.sigmoid(predictions.squeeze(1))\n",
        "\n",
        "        # update all values over 0.5 to 1, else 0\n",
        "        predictions = (predictions > 0.5).int()\n",
        "        return predictions, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "3e057e12",
      "metadata": {
        "id": "3e057e12"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Encapsulates the encoder and decoder. Pass a song matrix to the forward method, and it\n",
        "    will encode it, then output the decoder's prediction of it\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "  \n",
        "    def forward(self, x, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        pre: x is the song on host not device - the gpu runs out of memory\n",
        "             with the whole song\n",
        "        \"\"\"\n",
        "        # x: the song with shape [len, 90]\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(x.shape)\n",
        "        \n",
        "        # encoder input song\n",
        "        len_song = x.shape[0]\n",
        "\n",
        "        h,c = 0,0\n",
        "        # pass each row to the encoder one-by-one\n",
        "        for i in range(len_song):\n",
        "            current_row = x[i,:].to(device)\n",
        "            h,c = self.encoder(current_row.unsqueeze(0))\n",
        "\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = x[0,:].to(device)\n",
        "\n",
        "        # decode the length of the song and return prediction\n",
        "        for t in range(1, len_song):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, h, c = self.decoder(input, h, c)\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output.to(\"cpu\")\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = x[t,:].to(device) if teacher_force else output\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "    def generate_song(self, max_length):\n",
        "        \"\"\"\n",
        "        Generate a song based on the parameters of the encoder and decoder\n",
        "        Pre: encoder and decoder should be optimized\n",
        "        Return: The song as an song_length x 88 size numpy array\n",
        "        \"\"\"\n",
        "\n",
        "        num_notes = self.encoder.input_size\n",
        "\n",
        "        song_matrix = torch.zeros(max_length, num_notes, dtype=torch.int)\n",
        "\n",
        "        # encode start token\n",
        "        song_matrix[0, 0] = 1\n",
        "\n",
        "        # initial state is based on encoding the start token\n",
        "        input = song_matrix[0, :].to(device)\n",
        "\n",
        "        h, c = self.encoder(input.unsqueeze(0))\n",
        "\n",
        "        # Predict the notes of this song following inputting the start token\n",
        "        for t in range(1, max_length):\n",
        "            # input is the predicted set of notes, and the input to the next\n",
        "            # prediction\n",
        "            input, h, c = self.decoder(input, h, c)\n",
        "\n",
        "            song_matrix[t] = input.to(\"cpu\")\n",
        "\n",
        "            # if end token is predicted: end\n",
        "            if input[-1] == 0:\n",
        "                print(\"end token\")\n",
        "                break\n",
        "\n",
        "        print(\"t:\", t)\n",
        "        # the song generated until the end token (or max_length)\n",
        "        song_matrix = song_matrix[:t+1,:]\n",
        "        \n",
        "        # convert to np array\n",
        "        song_matrix = np.array(song_matrix)\n",
        "\n",
        "        # trim outer padding for start and end tokens\n",
        "        return song_matrix[1:-1, 1:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0ef1ec3",
      "metadata": {
        "id": "f0ef1ec3"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "5b60e717",
      "metadata": {
        "id": "5b60e717"
      },
      "outputs": [],
      "source": [
        "drop_p = 0.5\n",
        "\n",
        "# data has 88 columns for each piano note, plus two for start and end tokens\n",
        "midi_dim = 90\n",
        "\n",
        "# model hyperparameters\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "\n",
        "# training hyperparemeters\n",
        "num_epochs = 1\n",
        "num_songs = 1\n",
        "\n",
        "save_model = True\n",
        "load_model = False\n",
        "model_filename = \"model.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "2779875c",
      "metadata": {
        "id": "2779875c"
      },
      "outputs": [],
      "source": [
        "encoder = EncoderRNN(midi_dim, hidden_size, num_layers, drop_p)\n",
        "decoder = DecoderRNN(midi_dim, hidden_size, 1, num_layers, drop_p)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_songs = min(num_songs, len(os.listdir(csv_data_dir)))\n",
        "\n",
        "def train(model, num_songs):\n",
        "    \"\"\"\n",
        "    Pass num_songs to the model and optimize at each\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(num_songs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # load song as numpy array\n",
        "        midi_tensor = np.genfromtxt(dir_idx(csv_data_dir, i), delimiter=',', dtype='int')\n",
        "\n",
        "        # convert to tensor\n",
        "        midi_tensor = torch.IntTensor(midi_tensor)\n",
        "        \n",
        "        # decode prediction\n",
        "        predicted_song = model(midi_tensor)\n",
        "        predicted_song.requires_grad = True\n",
        "\n",
        "        # optimize\n",
        "        loss = criterion(predicted_song, midi_tensor.float())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "model_loaded = False\n",
        "\n",
        "# load the model\n",
        "if load_model and model_filename in os.listdir():\n",
        "    model.load_state_dict(torch.load(model_filename))\n",
        "    model_loaded = True\n",
        "\n",
        "# train the model\n",
        "else:\n",
        "    for epoch in range(num_epochs):\n",
        "        train(model, num_songs)\n",
        "\n",
        "if save_model and not model_loaded:\n",
        "    torch.save(model.state_dict(), model_filename)"
      ],
      "metadata": {
        "id": "JpTbCz7XDfm7"
      },
      "id": "JpTbCz7XDfm7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}