{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0a85f68a",
      "metadata": {
        "id": "0a85f68a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7bd96cb-f110-487e-8cbd-561a22ab2850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mido in /usr/local/lib/python3.7/dist-packages (1.2.10)\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# if using google colab - set up path properly\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    !pip install mido\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "\n",
        "    import sys\n",
        "    cwd = '/content/drive/My Drive/school/stat4984/final_proj/'\n",
        "    sys.path.append(cwd)\n",
        "\n",
        "else:\n",
        "    cwd = os.getcwd()+'/'\n",
        "\n",
        "\n",
        "import mido\n",
        "from midi_ndarrays import *\n",
        "\n",
        "# set up directories\n",
        "midi_data_dir = cwd+'midi_data/'\n",
        "csv_data_dir  = cwd+'csv_data/'\n",
        "\n",
        "if not os.path.exists(midi_data_dir):\n",
        "    os.makedirs(midi_data_dir)\n",
        "\n",
        "if not os.path.exists(midi_data_dir):\n",
        "    os.makedirs(csv_data_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c33f11e6",
      "metadata": {
        "id": "c33f11e6"
      },
      "outputs": [],
      "source": [
        "def dir_idx(dir_name, n):\n",
        "    \"\"\"\n",
        "    return the name of the nth file in a directory\n",
        "    \"\"\"\n",
        "    \n",
        "    return dir_name+os.listdir(dir_name)[n]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caed195a",
      "metadata": {
        "id": "caed195a"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dabc932d",
      "metadata": {
        "scrolled": true,
        "id": "dabc932d"
      },
      "outputs": [],
      "source": [
        "# only download midis if the batch dir doesn't exist and midi dir is empty\n",
        "if len(os.listdir(midi_data_dir)) == 0 and len(os.listdir(csv_data_dir)) == 0:\n",
        "    from midi_utils import download_midis\n",
        "    download_midis(midi_data_dir)\n",
        "\n",
        "\n",
        "def load_array(midi_filename):\n",
        "    \"\"\"\n",
        "    return midi_filename as a ndarray with start and end tokens\n",
        "    Replace all notes with 1 - played, or 0 - not played\n",
        "    \"\"\"\n",
        "    midi_tracks = mido.MidiFile(midi_filename, clip=True)\n",
        "    midi_array = mid2array(midi_tracks)\n",
        "    \n",
        "    # set all values to 1 or 0\n",
        "    midi_array = np.where(midi_array != 0, 1, 0).astype('uint8')\n",
        "    \n",
        "    # add padding and encode start token (first column)\n",
        "    # and end token (last column)\n",
        "    midi_array = np.pad(midi_array, 1)\n",
        "    midi_array[0, 0]   = 1\n",
        "    midi_array[-1, -1] = 1\n",
        "    \n",
        "    return midi_array\n",
        "\n",
        "# if there aren't any csv files, convert all midis arrays and save in csv\n",
        "if len(os.listdir(csv_data_dir)) == 0:\n",
        "    for i in range(len(os.listdir(midi_data_dir))):\n",
        "        midi_filename = dir_idx(midi_data_dir, i)\n",
        "        midi_array = load_array(midi_filename)\n",
        "        np.savetxt(midi_filename[:-4]+\".csv\", midi_array, fmt=\"%d\", delimiter=\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4f7a8b",
      "metadata": {
        "id": "1c4f7a8b"
      },
      "source": [
        "# The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8f0840d8",
      "metadata": {
        "id": "8f0840d8"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout_p):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers  = num_layers\n",
        "        \n",
        "        self.dropout   = nn.Dropout(dropout_p)\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn       = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout_p)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # input dimension [1, 90]\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        _, (h, c) = self.rnn(embedding)\n",
        "        return h, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "69d55259",
      "metadata": {
        "id": "69d55259"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, \n",
        "                num_layers, dropout_p):\n",
        "    \n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers  = num_layers\n",
        "\n",
        "        self.dropout   = nn.Dropout(dropout_p)\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        \n",
        "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout_p)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x, hidden, cell):\n",
        "        x = x.unsqueeze(0)\n",
        "        \n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "        predictions = self.fc(outputs.squeeze(0))\n",
        "        \n",
        "        # sigmoid because we're predicting multiple notes\n",
        "        predictions = torch.sigmoid(predictions.squeeze(1))\n",
        "\n",
        "        #predictions = predictions.squeeze(0)\n",
        "        \n",
        "        return predictions, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3e057e12",
      "metadata": {
        "id": "3e057e12"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Encapsulates the encoder and decoder. Pass a song matrix to the forward method, and it\n",
        "    will encode it, then output the decoder's prediction of it\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "  \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_out).to(device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0ef1ec3",
      "metadata": {
        "id": "f0ef1ec3"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5b60e717",
      "metadata": {
        "id": "5b60e717"
      },
      "outputs": [],
      "source": [
        "drop_p = 0.5\n",
        "\n",
        "# data has 88 columns for each piano note, plus two for start and end tokens\n",
        "midi_dim = 90\n",
        "\n",
        "# model hyperparameters\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "\n",
        "# training hyperparemeters\n",
        "num_epochs = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "2779875c",
      "metadata": {
        "id": "2779875c"
      },
      "outputs": [],
      "source": [
        "encoder = EncoderRNN(midi_dim, hidden_size, num_layers, drop_p)\n",
        "decoder = DecoderRNN(midi_dim, hidden_size, 1, num_layers, drop_p)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# TODO\n",
        "for epoch in range(num_epochs):\n",
        "    #for i in range(len(os.listdir()))\n",
        "        pass\n",
        "        # pass to seq to seq, compute loss, optimize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WIP\n",
        "\n",
        "Testing passing input to encoder and decoder on 10 rows of an example midi"
      ],
      "metadata": {
        "id": "FKVkOUbc6A72"
      },
      "id": "FKVkOUbc6A72"
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data_dir = cwd+\"csv_data/\"\n",
        "midi_array   = np.genfromtxt(dir_idx(csv_data_dir, 0), delimiter=',', dtype='int')\n",
        "\n",
        "midi_tensor = torch.IntTensor(midi_array).to(device)"
      ],
      "metadata": {
        "id": "VBP0Q1Nh5IUG"
      },
      "id": "VBP0Q1Nh5IUG",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = midi_tensor.shape[0]\n",
        "num_rows = 10\n",
        "\n",
        "h, c = 0, 0\n",
        "\n",
        "for i in range(num_rows):\n",
        "    # pass each row in the midi tensor to the encoder)\n",
        "    h, c = encoder(midi_tensor[i,:].unsqueeze(0))"
      ],
      "metadata": {
        "id": "SnAmOEou5DE0"
      },
      "id": "SnAmOEou5DE0",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred, h, c = decoder(midi_tensor[0,:], h, c)"
      ],
      "metadata": {
        "id": "xWKIKaEj8b5E"
      },
      "id": "xWKIKaEj8b5E",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO this is used in music generation step\n",
        "pred_cpu = torch.Tensor.cpu(pred)\n",
        "np.where(pred_cpu > 0.5, 1, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HEkc4Ww9k9Z",
        "outputId": "2b95ff92-dbab-44e5-eea9-bba693c5e050"
      },
      "id": "9HEkc4Ww9k9Z",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}